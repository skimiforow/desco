---
title: "DESCO - Knowledge Discovery - Prediction"
author: "1141074 - Sérgio Silva | 1970400 - Pedro Neves | 1040706 - Sérgio Castro"
date: "5/3/2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Limpeza do ambiente

```{r Clean Environment, include=FALSE}
rm(list = ls())
```

#Bibliotecas necessárias

```{r}
library(lattice)
library(ggplot2)
library(caret)
library(C50)
library(data.table)
library(randomForest)
library(rfm)
library(lubridate)
library(gmodels)
library(e1071) #Nayve Bayes
library(dplyr) # Para criação dos valores categoricos a partir dos dados números do RFM

```






#Carregamento das tabelas de transaction

The	 RFM	 (recency,	 frequency	 and	 monetary)	 model	 is	 the	 most	 widely	 used	 to	characterize customers	 because	 of its	 simplicity	and	good	 predictive	 capabilities.	 "Recency"	 represents	 the	time	since	the	last	purchase,	a	lower	value	corresponding	to	a	higher	probability	of	the	customer	making	a	repeat	 purchase.	 "Frequency"	 denotes	 the	 number	 of	 purchases	 within	 a	 specified	 time	 period;	 higher	frequency	indicates	higher	loyalty.	"Monetary"	means	 the	amount	of	money	spent	over this	specified	 time	period,	a	higher	value	indicates a	customer	that	the	company	should	focus on [2].

Loading of the tables to create model RFM
```{r Carregamento}
keycols <- c('Store', 'Date', 'Time', 'TransactionID')

transactions = fread('DATA-CRM/TRANSACTION.dat')
setkeyv(transactions, keycols)

trans_items = fread('DATA-CRM/TRANSACTION_ITEM.dat')
setkeyv(trans_items, keycols)

```

##Fazer 'join' das duas tabelas e remover observações contendo datas inválidas


```{r Merge}
Result <- merge(transactions, trans_items, all = TRUE)
Result[, Date := as.Date(as.character.Date(Date),"%Y%m%d")]
Result <- na.omit(Result, cols = "Date")
```


## Gerar RFM Score com package rfm

Calcula 'Recency', 'Frequency' e 'Monetary' a partir duma tabela de transações
```{r RFM algorithm}
analysis_date <- as.Date("2015-01-01")

orders <- Result[, .(CardID, Date, Amount)]
rfm_result <- rfm_table_order(orders, CardID, Date, Amount, analysis_date, recency_bins = 5, frequency_bins = 5, monetary_bins = 5)

summary(rfm_result)
str(rfm_result)

rfm_bar_chart(rfm_result)

rfm_histograms(rfm_result)

# Correlacionar a Frequência com o valor gasto
rfm_fm_plot(rfm_result)

# Correlacionar a data da última compra com a frequência de compra
rfm_rf_plot(rfm_result)

rfm_heatmap(rfm_result)
```

O valor de "Recency" é classificado numa escala de 1 a 5 em que o valor 5 é atribuido aos clientes que compraram mais recentemente.
O valor de "Frequency" usa uma escala de 1 a 5 para indicar com 5 o cliente que compra com mais frequência
Por fim, o valor de Monetary classifica, também com uma escala de 1 a 5, indicando com 5 os clientes que gastam mais valor.

As divisões dos cestos é feita com o valor por defeito de 5, que divide em os valores a cada 20% dos dados.

Caso fosse necessério o calculo dos atributos do algoritmo RFM, poderíamos usar as seguintes formulas. As mesmas não foram usadas pois usamos o package RFM que nos dá este calculo automaticamente.

Cálculo "Manual" das três dimensões do RFM
```{r}
# Frequency
rfm_frequency <- Result[, .N, by = CardID]

# Monetary
rfm_monetary <- Result[ , .(Total = sum(Amount)), by = CardID]

# Recency
rfm_recency <- Result[, .(LatestOrder = max(Date)), by = CardID][, .(Recency = analysis_date - LatestOrder), by = CardID]
```


#1. Exploração e preparação dos dados



##Tabela CAMPAIGN.DAT
Carregamento da tabela CAMPAIGN.dat
```{r CAMPAIGN.dat}
campaign <- read.table("DATA-CRM/CAMPAIGN.dat", header = TRUE, sep = ",", dec = ",")
```

### Verificação dos dados da tabela campaign, tal como o número de colunas e linhas, bem como se os dados foram corretamente carregados.
```{r}
summary(campaign)
dim(campaign)
str(campaign)
```

###Verificar se a tabela possui dados nulos
```{r}
table(is.na(campaign))
unique(campaign)
table(duplicated(campaign))

```


## Tabela CARD.dat
```{r CARD.dat}
card <- read.table("DATA-CRM/CARD.dat", header = TRUE, sep = ",", dec = ",")
setDT(card)
```

### Verificação dos dados da tabela card, tal como o número de colunas e linhas, bem como se os dados foram corretamente carregados.
```{r}
summary(card)
dim(card)
str(card)
nrow(card[!complete.cases(card),])
```

### Tratar os dados nulos e errados nas colunas NumChildren e YoungestChild , pois tem valores negativos. No MaritalStatus há dados nulos que devem ser tratados também.

```{r}
card$NumChildren[card$NumChildren < 0] <- 0
table(card$NumChildren)
card$YoungestChild[card$YoungestChild < 0] <- 0
table(card$YoungestChild)


# Como existem observações que indicam ter filhos, mas depois o campo do número de filhos está a 0, decidimos alterar o atributo "HasChildren" para "N"
card[card$NumChildren == 0 & card$HasChildren == 'Y', .N]
card$HasChildren[card$NumChildren == 0] <- 'N'


# NumChildren>0 e HasChildren
card[card$NumChildren > 0 & card$HasChildren == 'N', .N]
card$HasChildren[card$NumChildren > 0] <- 'Y'

plot(card$NumChildren~card$HasChildren)

#TODO: o que colocar em YoungestChild quando HasChildren == 'N' e NumChildren == 0? 


# Verificamos que o MaritalStatus possui dados vazios
table(card$MaritalStatus)

card[MaritalStatus == "", MaritalStatus := "O"]
card$MaritalStatus <- droplevels(card$MaritalStatus)
```

### Verificar se a tabela Card possui dados nulos
```{r}
table(is.na(card))
unique(card)
table(duplicated(card))

```


#2. Pré-processamento dos dados

## Merge das tabelas

```{r juntar dados dos clientes}
df_card <- merge(card, rfm_result$rfm[,c(1,9)], by.x = 'CardID', by.y = 'customer_id')

df_card <- merge(df_card, campaign, by = 'CardID')

df_card[, CardStartDate := as.Date(as.character.Date(CardStartDate),"%Y%m%d")]

df_card[, DateOfBirth := as.Date(as.character.Date(DateOfBirth),"%Y%m%d")]

```

```{r criar as colunas com idade e anos de cliente}
# Criar a idade do cliente
yr = duration(num = 1, units = "years")
df_card[, age := round(interval(DateOfBirth, analysis_date)/yr,digits = 0)][]

# Anos de clientes
df_card[, clientYears := round(interval(CardStartDate, analysis_date)/yr,digits = 0)][]

summary(df_card)
```

```{r retirar id e código postal, CardStartDate e DateOfBirth}

df_card <- df_card[, c(2,3,6,8:15)]

# Mudar o atributo objetivo para o fim da tabela
df_card <- df_card[, c(1:8,10,11,9)]

str(df_card)


df_card$Gender <- factor(df_card$Gender, levels = c("F", "M"), labels = c("Feminino", "Masculino"))
df_card$MaritalStatus <- factor(df_card$MaritalStatus, levels = c("M", "S", "O"), labels = c("Casado", "Solteiro", "Outro"))
df_card$HasChildren <- factor(df_card$HasChildren, levels = c("Y", "N"), labels = c("Sim","Não"))
df_card$Responded <- factor(df_card$Responded, levels = c(TRUE, FALSE), labels = c("Sim","Não"))

```


## Visualização dos dados

```{r}



# Verificar a percentagem de clientes que responderam à campanha
barplot(table(df_card$Responded), ylim = c(0,10000), main = "Responded")

prop.table(table(df_card$Responded))*100


```



#3. Criação de modelos de Data-Mining e avaliação dos mesmos


```{r Função para avaliação dos modelos e data frame para guardar os dados}


allcrosstabmeasures <- function (tst_labels, pred_labels, modelname=""){
  if (length(unique(tst_labels)) == length(unique(pred_labels))) {
    ctmatrix <- CrossTable(tst_labels,pred_labels,prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c("Real","Predicted"))
    accuracy <- sum(diag(ctmatrix$t))/sum(ctmatrix$t)
    recall <- ctmatrix[[1]][1,1]/sum(ctmatrix[[1]][1,]) #TPR Recall
    precision <- ctmatrix[[1]][1,1]/sum(ctmatrix[[1]][,1])
    fpr <- ctmatrix[[1]][2,1]/sum(ctmatrix[[1]][2,])
    f1 <- 2*precision*recall/(precision+recall)
    pre <- (sum(ctmatrix[[1]][1,])/sum(ctmatrix$t)*sum(ctmatrix[[1]][,1])/sum(ctmatrix$t)) + (sum(ctmatrix[[1]][2,])/sum(ctmatrix$t)*sum(ctmatrix[[1]][,2])/sum(ctmatrix$t))
  kappa <- (accuracy - pre) / (1-pre)
  data.frame(model=modelname,accuracy=round(accuracy,digits = 3),recall=round(recall,digits = 3),precision=round(precision,digits = 3),f1=round(f1,digits = 3), kappa=round(kappa,digits = 3),fpr=round(fpr,digits = 3))
  } else {
    stop("Erro na construção do modelo")
  }
}


# Declara um data frame para guardar os dados estatisticos

results.df <- data.frame(model = character(),
                         accuracy = double(),
                         recall = double(),
                         precision = double(),
                         f1 = double(),
                         kappa = double(),
                         fpr = double(),
                         stringsAsFactors = FALSE)

```


## Devido à taxa de Sim no atributo Responded ser inferior a 20%, foi necessário efetuar um Oversamppling dos dados positivos para equilibrar os mesmos.
```{r Holdout stratified}
# Holdout stratified
   
set.seed(123)

#create of stratified train/test partitions
train.idx <- createDataPartition(df_card$Responded, p = 0.7, list = F)

trainSet <- df_card[ train.idx, ]
tstSet <-   df_card[-train.idx, ]

# Criar o oversampling para resolver o problema dos dados desbalanceados.
oversampling <- trainSet[ which(trainSet$Responded=='Sim'),]
trainSet <- rbind(trainSet,oversampling)
trainSet <- rbind(trainSet,oversampling)
table(trainSet$Responded)
summary(trainSet)

table(trainSet$Responded)
round(prop.table(table(trainSet$Responded))*100, digits = 2)

train_labels <- trainSet$Responded
tst_labels <- tstSet$Responded

round(prop.table(table(train_labels))*100, digits = 2)
round(prop.table(table(tst_labels))*100, digits = 2)

ncols <- dim(trainSet)[2]
ctrl <- trainControl(method = "cv", number = 10)
```



```{r C5.0}
set.seed(123)

modelLookup(model = "C5.0") # ver o que o model pede

C5Model <- C5.0(Responded~.,data = trainSet)

summary(C5Model)

C5Predict <- predict(C5Model,newdata = tstSet,type = "class")

results.df <- rbind(results.df,allcrosstabmeasures(tst_labels,C5Predict,"C5.0 Classif."))
results.df
```


```{r C5.0 com funcao treino}
##################################################################################

# C5.0 com funcão treino


#grade de parametros para otimizar o algoritmo

grid <- expand.grid(winnow = c(TRUE,FALSE), trials = c(1,2,10,15,20), model = "tree")
grid


C5model <- train(Responded~.,data = trainSet,
                 method = 'C5.0',
                 metric = 'Accuracy',
                 trControl = ctrl,
                 tuneGrid = grid)
C5model

trellis.par.set(caretTheme())
plot(C5model)


C5pred <- predict(C5model, tstSet)

table(C5pred)

results.df <- rbind(results.df, allcrosstabmeasures(tst_labels, C5pred, "C5.0 with train caret "))
results.df
```



```{r C5.0 with adaptative boosting}
##################################################################################

# C5.0 with adaptative boosting

set.seed(123)

boost100.model <- C5.0(x = trainSet[,c(1:10)],trainSet$Responded, trials = 100)

summary(boost100.model)

boost100.pred <- predict(boost100.model,tstSet)

results.df <- rbind(results.df,allcrosstabmeasures(tst_labels,boost100.pred,"C5.0 with adaptative boosting "))
results.df

##################################################################################
```

Com a matrix de custos a ideia é tentar melhorar as previsões atribuindo custos a previsões menos desejadas.

```{r C5.0 com uma marix de custos para tentar melhor o resultado}

cost.matrix <- matrix(c(
  NA, 4,
  1, NA
), 2, 2, byrow=TRUE)

rownames(cost.matrix) <- colnames(cost.matrix) <- levels(trainSet$Responded)

cost.matrix

grid <- expand.grid(winnow = c(TRUE,FALSE), trials = c(1,2,10,15,20), model = "tree",cost = cost.matrix)
grid
set.seed(123)

C5model <- train(Responded~.,data = trainSet,
                 method = "C5.0Cost",
                 metric = "Accuracy",
                 trControl = ctrl,
                 tuneGrid = grid)

C5model

C5pred <- predict(C5model,tstSet)

results.df <- rbind(results.df,allcrosstabmeasures(tst_labels,C5pred,"C5.0 with costs "))
results.df

```



```{r Naive Bayes}
##################################################################################

# Naive Bayes

set.seed(123)

Bayesmodel <-train(trainSet[,c(1:10)], y = trainSet$Responded,
                    method = "nb",
                    tuneGrid = data.frame(fL=1,usekernel=FALSE,adjust=1),
                    trControl = ctrl)

Bayespred <- predict(Bayesmodel,tstSet)

results.df <- rbind(results.df,allcrosstabmeasures(tst_labels,Bayespred,"Naive Bayes"))
results.df

##################################################################################
```

```{r Random Forest}


set.seed(123)

control <- trainControl(method = "repeatedcv", number = 10, repeats = 3, search  ="grid")

mtry <- round(sqrt(ncol(trainSet)),0)

rfmodel <- train(Responded~.,
                 data=trainSet,
                 method = "rf",
                 tuneGrid = expand.grid(.mtry=1:15),
                 trControl=control,
                 ntree=100)


print(rfmodel)
plot(rfmodel)

varImp(rfmodel)

rfpredict <- predict(rfmodel,newdata = testSet,type = 'raw')
results.df <- rbind(results.df,allcrosstabmeasures(test_Labels,rfpredict,"Random Forest"))
results.df
```


# Logistic regression
```{r}
crtl <- trainControl(method = 'cv', number = 3)

set.seed(123)

glmodel <- train(Responded~.,
                 data= trainSet,
                 method = 'glm',
                 trControl = crtl)

summary(glmodel)

glmpred <- predict(glmodel, newdata = tstSet)
results.df <- rbind(results.df,allcrosstabmeasures(tst_labels,glmpred,"Logistic regression"))
results.df
```

#TODO: está a dar erro
# K-Nearest neighbours
```{r}
set.seed(123)

knnmodel <- train(Responded~.,
                  data = trainSet,
                  method = 'knn',
                  predProc = c("center","scale"),
                  tuneGrid = expand.grid(k = seq(1,100,2)),
                  metric = 'Accuracy',
                  trControl = crtl)

knnpred <- predict(knnmodel,newdata = tstSet, type = "raw")

results.df <- rbind(results.df,allcrosstabmeasures(tst_labels,knnpred,"K-Nearest neighbours"))
results.df
```


#TODO: está a dar erro
# Support Vector Machine (SVM) LINEAR
```{r}
set.seed(123)

svmodel <- train(Responded~.,
                 data = trainSet,
                 method = 'svmLinear',
                 metric = 'Accuracy',
                 maximize = FALSE,
                 tuneLength = 15,
                 trControl = crtl)

svpred <- predict(svmodel,newdata = tstSet)

results.df <- rbind(results.df,allcrosstabmeasures(tst_labels,svpred,"Support Vector Machine (SVM) - Linear"))
results.df
```


# Support Vector Machine (SVM) RADIAL
```{r}
set.seed(123)
grid <- expand.grid(sigma = c(.01,.015, 0.2), C = c(0.75, 0.9, 1,1.1,1.25))
svmodelradial <- train(Responded~.,
                 data = trainSet,
                 method = 'svmRadial',
                 PreProc = c("center","scale"),
                 metric = 'Accuracy',
                 maximize = FALSE,
                 tuneGrid = grid,
                 trControl = crtl)

svpredradial <- predict(svmodelradial,newdata = testSet)

results.df <- rbind(results.df,allcrosstabmeasures(test_Labels,svpredradial,"Support Vector Machine (SVM) - Radial"))
results.df
```

# neural network
```{r}
set.seed(123)

nnetGrid <- expand.grid(.decay = c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9), size = (1:9))

nnmodel <- train(Responded~.,
                 data = trainSet,
                 method = 'nnet',
                 metric = 'Accuracy',
                 tuneGrid = nnetGrid,
                 trace = FALSE,
                 maxit = 1000,
                 preProc = c("center","scale"),
                 trControl = crtl)

nnpred <- predict(nnmodel,newdata = testSet)

results.df <- rbind(results.df,allcrosstabmeasures(test_Labels,nnpred,"Neural network"))
results.df
```

#Analise do resultado dos modelos de classificação.

"True positive" e "true negative" são as observações que foram corretamente previstas. a ideia e minimizar os "false positive" e os "false negative", sendo que o mais importante é reduzir o risco de perder negócio, com a identificação de clientes que não iriam responder à campanha quando o resultado era positivo.

True Positives (TP) - Estes valores são valores da classe "Sim" que foram corretamente previstos.

True Negatives (TN) - São os valores da classe "Não" que foram corretamente previstos como "Não".

False positives and false negatives, these values occur when your actual class contradicts with the predicted class.

False Positives (FP) - When actual class is no and predicted class is yes. E.g. if actual class says this passenger did not survive but predicted class tells you that this passenger will survive.

False Negatives (FN) - When actual class is yes but predicted class in no. E.g. if actual class value indicates that this passenger survived and predicted class tells you that passenger will die.

Once you understand these four parameters then we can calculate Accuracy, Precision, Recall and F1 score.


Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model. For our model, we have got 0.803 which means our model is approx. 80% accurate.

Accuracy = TP+TN/TP+FP+FN+TN

Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. We have got 0.788 precision which is pretty good.

Precision = TP/TP+FP

Recall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we label? We have got recall of 0.631 which is good for this model as it's above 0.5.

Recall = TP/TP+FN

F1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it's better to look at both Precision and Recall. In our case, F1 score is 0.701.

F1 Score = 2*(Recall * Precision) / (Recall + Precision)

So, whenever you build a model, this article should help you to figure out what these parameters mean and how good your model has performed.


# Alterar o RFM para dados categoricos para avaliaçãodo tipo de cliente
```{r Criação de variáveis categóricas a partir dos dados numéricos do RFM}

table(df_card$rfm_score)
hist(df_card$rfm_score,breaks = seq(min(df_card$rfm_score), max(df_card$rfm_score), length.out = df_card$rfm_score))

summary(df_card)


df_card$rfm_score_cat <- case_when(df_card$rfm_score <= 233 ~ 'Sporadically',
                  between(df_card$rfm_score, 234, 443) ~ 'Regular',
                  df_card$rfm_score >= 444 ~ 'Frequent'
                  )

df_card$rfm_score_cat <- as.factor(df_card$rfm_score_cat)

summary(df_card)

new_df_card <- df_card[,c(1:7,9,10,12)]
attach(new_df_card)

```

# Declarar um data frame para guardar os dados estatisticos

```{r}

results.new_df <- data.frame(model = character(),
                         accuracy = double(),
                         recall = double(),
                         precision = double(),
                         f1 = double(),
                         kappa = double(),
                         fpr = double(),
                         stringsAsFactors = FALSE)
```


## Holdout stratified
```{r Holdout stratified from second classification}


set.seed(123)

#create of stratified train/test partitions
train.idx <- createDataPartition(new_df_card$rfm_score_cat, p = 0.7, list = F)

trainSet <- new_df_card[ train.idx, ]
tstSet <-   new_df_card[-train.idx, ]

table(trainSet$rfm_score_cat)
round(prop.table(table(trainSet$rfm_score_cat))*100, digits = 2)

train_labels <- trainSet$rfm_score_cat
tst_labels <- tstSet$rfm_score_cat

round(prop.table(table(train_labels))*100, digits = 2)
round(prop.table(table(tst_labels))*100, digits = 2)

ncols <- dim(trainSet)[2]
ctrl <- trainControl(method = "cv", number = 10)
```




```{r C5.0 second classification}
set.seed(123)

modelLookup(model = "C5.0") # ver o que o model pede

C5Model <- C5.0(rfm_score_cat~.,data = trainSet)

summary(C5Model)

C5Predict <- predict(C5Model,newdata = tstSet,type = "class")

results.new_df <- rbind(results.new_df,allcrosstabmeasures(tst_labels,C5Predict,"C5.0 Classif."))
results.new_df
```

```{r C5.0 with adaptative boosting second classification}
set.seed(123)

boost100.model <- C5.0(rfm_score_cat~.,data = trainSet, trials = 100)

summary(boost100.model)

boost100.pred <- predict(boost100.model,tstSet)

results.new_df <- rbind(results.new_df,allcrosstabmeasures(tst_labels,boost100.pred,"C5.0 with adaptative boosting "))
results.new_df
```





