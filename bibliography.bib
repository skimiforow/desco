@article{Birant2011,
abstract = {The progress of data mining technology and large public popularity establish a need for a comprehensive text on the subject. The series of books entitled by 'Data Mining' address the need by presenting in-depth description of novel mining algorithms and many useful applications. In addition to understanding each section deeply, the two books present useful hints and strategies to solving problems in the following chapters. The contributing authors have highlighted many future research directions that will foster multi-disciplinary collaborations and hence will lead to significant development in the field of data mining.},
author = {Birant, Derya},
doi = {10.5772/13683},
file = {:C$\backslash$:/Users/skimi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Birant - Unknown - Data Mining Using RFM Analysis.pdf:pdf},
isbn = {978-953-307-154-1},
journal = {Knowledge-Oriented Applications in Data Mining},
keywords = {RFM},
mendeley-groups = {DESCO},
number = {iii},
pages = {91--108},
title = {{Data Mining Using RFM Analysis}},
url = {http://cdn.intechopen.com/pdfs/13162/InTech-Data{\_}mining{\_}using{\_}rfm{\_}analysis.pdf http://www.intechopen.com/books/knowledge-oriented-applications-in-data-mining/data-mining-using-rfm- analysis},
year = {2011}
}
@article{Fan2016,
author = {Fan, Yi},
mendeley-groups = {DESCO},
title = {{RFM Analysis in R MATH 3201 DATAMINING FOUNDATION}},
url = {http://mercury.webster.edu/aleshunas/Support Materials/Analysis/Fan-Yi Fan{\_}MATH3210{\_}Final Project 1.pdf},
year = {2016}
}
@article{Marbac2017,
abstract = {Variable selection in cluster analysis is important yet challenging. It can be achieved by regularization methods, which realize a trade-off between the clustering accuracy and the number of selected variables by using a lasso-type penalty. However, the calibration of the penalty term can suffer from criticisms. Model selection methods are an efficient alternative, yet they require a difficult optimization of an information criterion which involves combinatorial problems. First, most of these optimization algorithms are based on a suboptimal procedure (e.g. stepwise method). Second, the algorithms are often computationally expensive because they need multiple calls of EM algorithms. Here we propose to use a new information criterion based on the integrated complete-data likelihood. It does not require the maximum likelihood estimate and its maximization appears to be simple and computationally efficient. The original contribution of our approach is to perform the model selection without requiring any parameter estimation. Then, parameter inference is needed only for the unique selected model. This approach is used for the variable selection of a Gaussian mixture model with conditional independence assumed. The numerical experiments on simulated and benchmark datasets show that the proposed method often outperforms two classical approaches for variable selection. The proposed approach is implemented in the R package VarSelLCM available on CRAN.},
archivePrefix = {arXiv},
arxivId = {1501.06314},
author = {Marbac, Matthieu and Sedki, Mohammed},
doi = {10.1007/s11222-016-9670-1},
eprint = {1501.06314},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Gaussian mixture model,Information criterion,Integrated complete-data likelihood,Model-based clustering,Variable selection},
mendeley-groups = {DESCO},
month = {jul},
number = {4},
pages = {1049--1063},
title = {{Variable selection for model-based clustering using the integrated complete-data likelihood}},
url = {http://link.springer.com/10.1007/s11222-016-9670-1},
volume = {27},
year = {2017}
}
@article{Huang1998,
abstract = {The k-means algorithm is well known for its efficiency in clustering large data sets. However, working only on numeric values prohibits it from being used to cluster real world data containing categorical values. In this paper we present two algorithms which extend the k-means algorithm to categorical domains and domains with mixed numeric and categorical values. The k-modes algorithm uses a simple matching dissimilarity measure to deal with categorical objects, replaces the means of clusters with modes, and uses a frequency-based method to update modes in the clustering process to minimise the clustering cost function. With these extensions the k-modes algorithm enables the clustering of categorical data in a fashion similar to k-means. The k-prototypes algorithm, through the definition of a combined dissimilarity measure, further integrates the k-means and k-modes algorithms to allow for clustering objects described by mixed numeric and categorical attributes. We use the well known soybean disease and credit approval data sets to demonstrate the clustering performance of the two algorithms. Our experiments on two real world data sets with half a million objects each show that the two algorithms are efficient when clustering large data sets, which is critical to data mining applications.},
author = {Huang, Zhexue},
doi = {10.1023/A:1009769707641},
isbn = {1384-5810},
issn = {1573-756X},
journal = {Data Mining and Knowledge Discovery},
keywords = {categorical data,cluster analysis,clustering algorithms,data mining},
mendeley-groups = {DESCO},
pages = {283--304},
title = {{Extensions to the k -Means Algorithm for Clustering Large Data Sets with Categorical Values}},
url = {http://www.cs.ust.hk/{~}qyang/Teaching/537/Papers/huang98extensions.pdf},
volume = {304},
year = {1998}
}
